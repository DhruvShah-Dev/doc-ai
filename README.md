Hereâ€™s your refined **README.md** with improved structure, clarity, and technical depth:

---

# **ğŸ“„ Document-Based AI Assistant**  
**A local, privacy-first AI chatbot that answers questions using your documentsâ€”no cloud APIs, no data leaks.**  

Powered by **Phi-2** (optimized for CPU) and **FAISS** for semantic search, this tool extracts insights from PDFs, DOCX, and TXT files while keeping all data on your machine.  

---

## **âœ¨ Key Features**  
| Feature | Description |  
|---------|-------------|  
| **ğŸ“‚ Document Support** | PDF, DOCX, TXT (text-based only) |  
| **ğŸ” Semantic Search** | FAISS + `all-MiniLM-L6-v2` embeddings for relevancy ranking |  
| **ğŸ¤– Local LLM** | Quantized Phi-2 (4-bit GGUF) via `llama.cpp` |  
| **ğŸš« No GPU Needed** | Runs efficiently on CPU (16GB RAM recommended) |  
| **ğŸŒ Responsive UI** | React frontend with drag-and-drop uploads |  

---

## **âš™ï¸ Tech Stack**  
### **Backend (Python)**  
- **Framework**: FastAPI  
- **Embeddings**: Sentence Transformers (`all-MiniLM-L6-v2`)  
- **Vector DB**: FAISS (local)  
- **Text Extraction**: `pdfplumber`, `python-docx`  
- **LLM**: Phi-2 (4-bit quantized via `llama.cpp`)  

### **Frontend (JavaScript)**  
- **UI**: React + Tailwind CSS  
- **Build**: Vite (optional)  

---

## **ğŸš€ Installation**  
### **1. Clone & Setup**  
```bash  
git clone https://github.com/DhruvShah-Dev/doc-ai.git  
cd doc-ai  
```  

### **2. Backend Setup**  
```bash  
pip install -r backend/requirements.txt  
```  
**Download Phi-2 Model**:  
```bash  
wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf -O backend/models/phi-2.Q4_K_M.gguf  
```  

### **3. Run the System**  
**Backend (FastAPI)**:  
```bash  
cd backend && uvicorn main:app --reload  
```  
**Frontend (React)**:  
```bash  
cd frontend && npm install && npm run dev  
```  

---

## **ğŸ“‚ Project Structure**  
```  
doc-ai/  
â”œâ”€â”€ backend/                  # FastAPI server  
â”‚   â”œâ”€â”€ models/               # Model to generate answers
        â”œâ”€â”€__init__.py
        â””â”€â”€llm_model.py
â”‚   â”œâ”€â”€ document_store/       # Text processing & FAISS
        â”œâ”€â”€__init__.py
        â”œâ”€â”€text_extractor.py
        â””â”€â”€vector_store.py
    â”œâ”€â”€models_folder/          # Phi-2 GGUF model will be downloaded here
    â”œâ”€â”€uploaded_files/         # All the uploaded files will be present here
    â”œâ”€â”€requirements.txt        # Download these before starting
â”‚   â””â”€â”€ main.py               # API endpoints  
â”œâ”€â”€ frontend/                 # React app  
â”‚   â”œâ”€â”€ public/               # Static assets  
â”‚   â””â”€â”€ src/                  # UI components  
â””â”€â”€ .gitignore                # Excludes models_folder/  
```  

---

## **ğŸ”§ Configuration**  
### **Backend (`backend/main.py`)**  
| Setting | Purpose | Default |  
|---------|---------|---------|  
| `UPLOAD_FOLDER` | Document storage path | `uploaded_files/` |  
| `MAX_FILE_SIZE_MB` | File size limit | `10` |  

### **LLM (`backend/models/llm_model.py`)**  
| Parameter | Effect | Recommended |  
|-----------|--------|-------------|  
| `n_ctx` | Context window | `2048` |  
| `temperature` | Answer creativity | `0.3` (factual) |  

---

## **ğŸ› ï¸ How It Works**  
1. **Upload** â†’ Documents are split into chunks and vectorized.  
2. **Query** â†’ FAISS retrieves top-3 relevant passages.  
3. **Answer** â†’ Phi-2 generates responses from context.  

**Example Queries**:  
- *â€œSummarize the key points.â€*  
- *â€œWhatâ€™s the conclusion of this paper?â€*  

---

## **âš ï¸ Limitations**  
- **No OCR**: Scanned PDFs unsupported.  
- **Hardware**: Requires â‰¥8GB RAM for smooth operation.  

---

## **ğŸ“œ License**  
MIT License.  
**Contact**: [@DhruvShah-Dev](https://github.com/DhruvShah-Dev)  

---

**Deploy locally and own your data!** ğŸ”’ğŸš€  
